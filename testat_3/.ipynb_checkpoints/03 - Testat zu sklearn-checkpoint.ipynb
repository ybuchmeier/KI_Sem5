{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "egyptian-worst",
   "metadata": {},
   "source": [
    "Wahlpflichtfach Künstliche Intelligenz II: Praktikum | [Startseite](index.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-theme",
   "metadata": {},
   "source": [
    "# 03 - Testat zu Scikit-learn (sklearn)\n",
    "__Gruppennummer:__ 3\n",
    "\n",
    "__Mitglieder:__\n",
    "- Jan Neitzner\n",
    "- Lukas Hein\n",
    "- Timo Marzok\n",
    "- Yannick Buchmeier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-adjustment",
   "metadata": {},
   "source": [
    "In diesem Testat werden Sie die unterschiedlichen Arbeitsschritte von der Datenvorverarbeitung über die Modell- und Teststrategieauswahl bis hin zur Evaluierung mit Hilfe von Scikit-learn durchführen. Dabei verwenden wir eine leicht modifizierte Variante des [California Housing Datasets](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). Dieses enthält die folgenden _acht_ Merkmale:\n",
    "- __MedInc:__ Das mittlere Einkommen im Block\n",
    "- __HouseAge:__ Das mittlere Hausalter im Block\n",
    "- __AveRooms:__ Die durchschnittliche Raumanzahl pro Haushalt im Block\n",
    "- __AveBedrms:__ Die durchschnittliche Schlafzimmeranzahl pro Haushalt im Block\n",
    "- __Population:__ Die Bevölkerunganzahl im Block\n",
    "- __AveOccup:__ Die durchschnittliche Anzahl von Personen pro Haushalt im Block\n",
    "- __Latitude:__ Der Breitengrad des Blocks\n",
    "- __Longitude:__ Der Längengrad des Blocks\n",
    "\n",
    "Jedem Datenpunkt ist genau einer Klasse (_low_, _mid-low_, _mid_, _mid-high_, _high_) zugeordnet, die angibt, wie hoch der mittlere Hauswert im Block ist. Jede Klasse enthält ~20% der Datenpunkte.\n",
    "\n",
    "## Aufgabe 0 - Data Understanding\n",
    "__unbenotet__\n",
    "\n",
    "Laden Sie die Daten und machen Sie sich mit ihnen vertraut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv('california_housing_data.csv')\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-opening",
   "metadata": {},
   "source": [
    "Eventuell hilft Ihnen auch der folgende Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-animal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sns.set_theme(style=\"whitegrid\")\n",
    "#sns.pairplot(housing_df, hue=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c632add",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"Label\"] = housing_df[\"Label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-drinking",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - Data Preparation (4 Punkte)\n",
    "Die erste Aufgabe ist es den Datensatz fürs maschinelle Lernen vorzubereiten. Dazu sind die folgenden Schritte nötig:\n",
    "- a) Auswahl der Strategie(n) zum Ersetzen der fehlenden Werte\n",
    "- b) Auswahl der Strategie(n) zur Skalierung der Daten\n",
    "- c) Erstellen der Preparation-Pipeline\n",
    "\n",
    "Da das Ersetzen der fehlenden Werte und die Skalierung der Daten in einer `Pipeline` passieren soll, können Sie nur Algorithmen verwenden, die __sklearn__ bereitstellt.\n",
    "\n",
    "_Hinweise/Tipps:_ \n",
    "- Sie müssen die unterschiedlichen Algorithmen nicht (bis zum Maximum) optimieren, hier geht es gerade eher darum zu überprüfen, ob Sie die Algorithmen generell verstanden haben und Sie richtig einsetzen/kombinieren können.\n",
    "- Gucken Sie sich nochmal die besprochenen Algorithmen an und überlegen wo die Stärken und Schwächen liegen.\n",
    "- Sie können selbstverständlich auch unterschiedliche Methoden für die einzelnen Merkmale wählen.  \n",
    "\n",
    "### a) Auswahl der Strategie(n) zum Ersetzen der fehlenden Werte\n",
    "_Punkte: 1_\n",
    "\n",
    "Als erstes müssen Sie sich eine Strategie zum Ersetzen der fehlenden Werte überlegen. Beschreiben Sie diese in der nachfolgenden Markdown-Zeile und begründen Sie, warum Sie diese Strategie gewählt haben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-wrong",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Zum Füllen von fehlenden Werten verwenden wir den ``SimpleImputer()``, der den Durchschnitt (arithmetisches Mittel) aus den vorhandenen Werten berechnet.\n",
    "\n",
    "Alternativ kann der ``IterativeImputer()`` verwendet werden. Dann müssten die Daten pro Label gruppiert werden und dann wird der Wert berechnet. Dadurch können unter Umständen andere Werte zustandekommen. Wir haben uns dagegen entschieden, weil so eine Korrelation erzwungen werden könnte, die es \"in echt\" nicht gibt.\n",
    "\n",
    "Gegen den ``OneHotEncoder`` spricht, dass nur das Label vom Typ Object ist.\n",
    "\n",
    "Der ``mean_imputer`` für fehlende geographische Angaben (``long`` und ``lat``) ist wenig zielführend, da es sich eigentlich um ordinale Daten gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-tension",
   "metadata": {},
   "source": [
    "### b) Auswahl der Strategie(n) zur Skalierung der Daten\n",
    "_Punkte: 1_\n",
    "\n",
    "Außerdem sollten die Daten skaliert/normalisiert werden. Beschreiben Sie Ihre Strategie und begründen Sie warum Sie diese Strategie bzw. Methoden gewählt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-contamination",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Der ``StandardScaler()`` kann auf Daten angewendet werden, die normalverteilt sind. Da viele Faktoren den Hauswert sowohl positiv als auch negativ beeinflussen und die Normalverteilung häufig vorkommen (das spiegelt sich im Namen mit ``Standard`` wider), haben wir uns dafür entschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-communication",
   "metadata": {},
   "source": [
    "### c) Erstellen der Preparation-Pipeline\n",
    "_Punkte: 2_\n",
    "\n",
    "In der nächsten Codezeile können Sie nun die `preparation_pipeline` erstellen. In dieser sollen beide vorherigen Schritte enthalten sein. Sie müssen die Pipeline aber noch nicht \"trainieren\" (Aufruf der Methode `fit()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "# Hier können Sie die weiteren benötigten Imports hinzufügen\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pipe = Pipeline([\n",
    "    (\"mean_imputer\", SimpleImputer()),\n",
    "    (\"scale\", StandardScaler())\n",
    "]\n",
    ")\n",
    "# IHRE LÖSUNG HIER\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-softball",
   "metadata": {},
   "source": [
    "Warum macht es noch keinen Sinn die Pipeline jetzt schon zu trainieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-premises",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Um ein ML-Modell zu erstellen, werden Trainings- und Testdatensätze benötigt. Nur so kann der ML-Algorithmus und dessen Performance ausgewertet werden. Hierfür muss der vorliegende Datensatz (``housing_df``) zunächst entsprechend unterteilt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-spyware",
   "metadata": {},
   "source": [
    "__Muster-Antwort:__\n",
    "- Wir haben noch keine separaten Trainings- und Testdatensets und daher würden wir Testdaten zum Training verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-alabama",
   "metadata": {},
   "source": [
    "## Aufgabe 2 - Trainingsvorbereitung und Modellauswahl  (4 Punkte)\n",
    "Ihre nächste Aufgabe ist es das Training vorzubereiten und den richtigen ML-Algorithmus auszuwählen. Dafür müssen Sie die folgenden Teilaufgaben erledigen:\n",
    "- a) Erstellen des Test- und Trainingsdatenset\n",
    "- b) Kreuzvalidierung im Trainingsprozess\n",
    "- c) Optimieren eines ML-Algorithmus\n",
    "- d) Testen der trainierten Pipeline\n",
    "\n",
    "### a) Erstellen des Test- und Trainingsdatenset\n",
    "_Punkte: 0,5_\n",
    "\n",
    "Zuerst benötigen wir ein Test- und ein Trainingsdatenset. Das Testdatenset soll 30% der gesamten Daten enthalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and target from the data frame \n",
    "data = housing_df.loc[:, :'Longitude']\n",
    "target = housing_df['Label']\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# IHRE LÖSUNG HIER\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Die Funktion train_test_split() teilt die gegebenen Parameter in zufälliger Reihenfolge entsprechend der Testgröße.\n",
    "train_data, test_data, train_label, test_label = train_test_split(data,\n",
    "                                                                  target, \n",
    "                                                                  test_size = 0.3 #30% der Daten werden als Testdaten genutzt\n",
    "                                                                 )\n",
    "\n",
    "#Überprüfen, ob die Größe der Daten korrekt ist.\n",
    "assert round(len(data)*0.3) == len(test_data)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-jones",
   "metadata": {},
   "source": [
    "### b) Kreuzvalidierung im Trainingsprozess\n",
    "_Punkte: 1_\n",
    "\n",
    "Was ist unter Kreuzvalidierung im Trainingsprozess zu verstehen und wieso wird es verwendet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-hampton",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Durch eine Pipeline können verschiedene Berechnungen verkettet werden, wodurch sie einfacher wiederholt durchzuführen sind. Die Berechnungen können hierbei durch verschiedene Parameter (Hyperparameter) angepasst werden. Da oft die Parameter für die beste Vorhersageperformance nicht bekannt sind, müssen diese durchprobiert werden. <br>\n",
    "Um die Performance zu messen und zu vergleichen wird der Datensatz, der zum Trainieren des ML-Modells genutzt wird, erneut aufgeteilt. Ein Bereich wird weiter für das Training des Modells benutzt, während der Rest zur Validierung genutzt wird. Da die Label-Daten des Validierungsdatensatzes bekannt sind, lässt sich hierraus die Genauigkeit der Vorhersagen berechnen.<br>\n",
    "Ein Problem durch dieses Verfahren ist die Verkleinerung des Trainingsdatensatzes. Dem wird durch die Kreuzvalidierung entgegengewirkt. Der Trainingsdatensatz wird n mal in n-Teile aufgesplittet. Für jeden n-ten Durchgang wird der n-te Teil des Datensatzes als Validierungsdatensatz benutzt, während der verbliebene Datensatz zum Training des Modells genutzt werden kann. Durch die Iteration der verschiedenen Datensätze in Kombination mit den verschiedenen Pipeline-Parametern können die optimalen Parameter herausgefunden werden.\n",
    "\n",
    "Innerhalb Scikit ist die Kreuzvalidierung durch sklearn.model_selection.GridSearchCV möglich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-macintosh",
   "metadata": {},
   "source": [
    "__Muster-Antwort:__\n",
    "- Während des Trainings müssen unterschiedliche Hyperparameter optimiert werden.\n",
    "- Um die optimalen Parameter auswählen zu können, muss auch getestet werden.\n",
    "- Daher wird ein weiteres Datenset benötigt, das sogenannte Validierungsdatenset.\n",
    "- Da dadurch die Daten fürs Training weiter schrumpfen, wurde die Kreuzvalidierung erfunden.\n",
    "- Splitten der Trainingsdaten in $k$ Subsets. Das Netzwerk $k$ mal trainieren und dabei immer mit dem $k$ten Subset testen. Durchschnitt der Testergebnisse ist Gesamtergebnis für eine bestimmte Hyperparameterkombination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-dating",
   "metadata": {},
   "source": [
    "### c) Optimieren eines ML-Algorithmus\n",
    "_Punkte: 1,5_\n",
    "\n",
    "Im nächsten Schritt optimieren wir einen ML-Algorithmus. Da wir ein Klassifikationsproblem lösen wollen, kommen nur Klassifikationsalgorithmen als mögliche Algorithmen in Frage. Wir werden den [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) verwenden. \n",
    "\n",
    "Der `RandomForestClassifier` soll eine maximale Tiefe von 6 haben und maximal 75% der Daten pro Baum verwenden. Setzen Sie den `random_state` auf 0. Für die Anzahl der Bäume sollen die folgenden Werte überprüft werden: `[30, 40, 50, 60, 70, 80]`. Außerdem sollen die beiden Möglichkeiten `['gini', 'entropy']` für das Kriterium, nach dem geteilt wird, getestet werden.\n",
    "\n",
    "Führen Sie die folgenden Schritte durch:\n",
    "- Erstellen Sie eine Pipeline, die zuerst die vorher bereits erstellte Vorverarbeitung durchführt und anschließend den `RandomForestClassifier` aufruft. \n",
    "- Finden Sie die optimalen Parameter aus den angegebenen Parameterbereichen.\n",
    "- Speichern Sie die `Pipeline` mit den besten Parametern in der Variable `trained_pipeline`\n",
    "- Geben Sie die beste `Pipeline` aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Hier können Sie weitere benötigte Importe hinzufügen\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pipe = Pipeline([\n",
    "                (\"mean_imputer\", SimpleImputer()), #NaN Werte mit Mittelwert befüllen\n",
    "                #(\"mean_imputer\", IterativeImputer()), #NaN Werte mit Mittelwert befüllen\n",
    "                (\"scale\", StandardScaler()),\n",
    "                (\"classifier\", RandomForestClassifier(\n",
    "                    max_depth=6, #maximale Tiefe von 6\n",
    "                    max_samples=0.75, #maximal 75% der Daten pro Baum\n",
    "                    random_state=0))\n",
    "])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd553409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(estimator=pipe,\n",
    "                     param_grid={\n",
    "                         'classifier__criterion':['gini', 'entropy'],\n",
    "                         'classifier__n_estimators' : [30, 40, 50, 60, 70, 80]\n",
    "                     },\n",
    "                     cv=3 #Cross Validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4db2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_label)\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a4ea4",
   "metadata": {},
   "source": [
    "Der beste Wert für den Parameter 'citerion' ist 'entropy'. <br>\n",
    "Der beste Wert für den Parameter 'n_estimators' ist 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fba43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-undergraduate",
   "metadata": {},
   "source": [
    "### d) Testen der trainierten Pipeline\n",
    "_Punkte: 1_\n",
    "\n",
    "Nachdem Sie die `Pipeline` trainiert haben, ist es nun Zeit diese zu testen. Lassen Sie sich dafür den Score einmal für das Test- und einmal für das Trainingsdatenset berechnen. Was fällt auf? Welche Metrik wird für das berechnen verwendet bzw. was sagt sie aus? Ist diese Metrik hier sinnvoll?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-thought",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# IHRE LÖSUNG HIER\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model und trained_pipline liefern das selbe Ergebnis\n",
    "predict_train = model.predict(train_data)\n",
    "predict_test = model.predict(test_data)\n",
    "print(accuracy_score(train_label, predict_train))\n",
    "print(accuracy_score(test_label, predict_test))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-childhood",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Mithilfe des ``accuracy_score`` kann berechnet werden, zu wie viel Prozent die beiden \"Listen\" übereinstimmen. Wenn eine Liste die gegebene, richtige Antwort und die zweite Liste die Vorhersage des Models ist, dann kann so die Genauigkeit bestimmt werden.\n",
    "\n",
    "Das Model ist auf die eher auf die Trainingsdaten angepasst, da es diese \"kennt\". Daher liegt hier prinzipiell eher ein Overfitting vor, sodass dieser Score wenig aussagekräftig ist.\n",
    "\n",
    "Der Test-Score zeigt, dass das Model zu etwa 50 % richtig liegt, womit es die Genauigkeit von Raten hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-classic",
   "metadata": {},
   "source": [
    "## Aufgabe 3 - Weitere Evaluierung und Visualisierung (2 Punkte)\n",
    "Im letzten Schritt wollen wir uns die Ergebnisse noch einmal genauer angucken, um eventuell zu verstehen, was passiert ist. Dazu sind die folgenden Teilaufgaben zu erledigen:\n",
    "- a) Erstellen eines Confusion Matrix-Diagramms\n",
    "- b) Analyse des Einfluss des Parameters n_estimators auf das Ergebnis\n",
    "\n",
    "### a) Erstellen eines Confusion Matrix-Diagramms\n",
    "_Punkte: 1_\n",
    "\n",
    "Erstellen Sie mit Hilfe der `plot_confusion_matrix`-Funktion das Diagramm der Confusion Matrix. Analysieren Sie dieses anschließend.\n",
    "\n",
    "_Tipps:_\n",
    "- Welche Klassen wurden falsch klassifiziert?\n",
    "- Was könnten mögliche Gründe dafür sein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "labels = ['low', 'mid-low', 'mid', 'mid-high', 'high']\n",
    "\n",
    "#Confusion Matrix mit Trainingsdaten\n",
    "_ = plot_confusion_matrix(estimator=model, #trainiertes Modell, das zur Vorhersage genutzt wrid\n",
    "                          X=train_data, #Daten für das Vorhersagen der Werte\n",
    "                          y_true=train_label, # erwartete Werte der Vorhersagen\n",
    "                          cmap='binary', # Farbschema\n",
    "                          labels=labels) #Sortierung der Achsen)\n",
    "\n",
    "#Confusion Matrix mit Testdaten\n",
    "_ = plot_confusion_matrix(estimator=model,\n",
    "                          X=test_data,\n",
    "                          y_true=test_label,\n",
    "                          cmap='binary',\n",
    "                          labels=labels)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-proposition",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Ein möglicher Grund ist die geringe Datenmenge, die als Grundlage zum Lernen und Testen verwendet wurde. Mit mehr Daten kann genauer bestimmt werden, wie die Korrelationen sind.\n",
    "\n",
    "Insbesondere Daten, die nebeneinander liegen, sind ungenau. So wurden Zeilen fälschlicherweise als low klassifiziert, wenn das tatsächliche Label jedoch mid-low ist. In der Grafik ist dies erkennbar, dass es mehr Grautöne gibt und nicht eine schwarze, trennscharfe Linie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-kruger",
   "metadata": {},
   "source": [
    "### b) Analyse des Einfluss des Parameters n_estimators auf das Ergebnis\n",
    "_Punkte: 1_\n",
    "\n",
    "Zum Schluss wollen wir noch einmal analysieren welchen Einfluss der Parameter `n_estimators` auf das Trainingsergebnis hat. Verwenden Sie dafür die Funktion `validation_curve`, um für den getesteten Parameterbereich des Parameters die nötigen Daten zu sammeln. Speichern Sie die Rückgabe in den Variablen `train_scores` und `valid_score`. Erstellen Sie außerdem eine Variable `n_estimators`, die alle möglichen Werte für den Parameter enthält. \n",
    "\n",
    "Welche Rückschlüsse lässt das Diagramm zu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "n_estimators = [30, 40, 50, 60, 70, 80]\n",
    "\n",
    "train_scores, valid_scores = validation_curve(estimator=pipe,\n",
    "                X=train_data,\n",
    "                y=train_label,\n",
    "                param_name='classifier__n_estimators',\n",
    "                param_range=n_estimators)\n",
    "# IHRE LÖSUNG HIER\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, calculate the means and standard deviations\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "valid_scores_mean = valid_scores.mean(axis=1)\n",
    "valid_scores_std = valid_scores.std(axis=1)\n",
    "\n",
    "# create the figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Validation Curve of RandomForestClassifier with different n_estimators values\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Score\")\n",
    "# plot the training score\n",
    "plt.plot(n_estimators, train_scores_mean, label=\"Training score\", color=\"darkorange\")\n",
    "plt.fill_between(n_estimators, \n",
    "                 train_scores_mean - train_scores_std, \n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 color=\"darkorange\",\n",
    "                 alpha=0.2\n",
    "                )\n",
    "# plot the validation score\n",
    "plt.plot(n_estimators, valid_scores_mean, label=\"Cross-validation score\", color=\"navy\")\n",
    "plt.fill_between(n_estimators, \n",
    "        \n",
    "                 valid_scores_mean - valid_scores_std, \n",
    "                 valid_scores_mean + valid_scores_std, \n",
    "                 color=\"navy\",\n",
    "                 alpha=0.2\n",
    "                )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-wages",
   "metadata": {},
   "source": [
    "__Ihre Antwort:__\n",
    "\n",
    "Es ist ein Trend zu erkennen: mit höheren ``n_estimators`` nimmt der Training-Score zu wohingegen der Validierung-Score abnimmt. Es kann also zu einem Overfitting führen. Wie zu erwarten liegt der Score für die Trainingsdaten höher, außerdem ist die Streuung sehr viel niedriger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-digest",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wahlpflichtfach Künstliche Intelligenz II: Praktikum | [Startseite](index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
