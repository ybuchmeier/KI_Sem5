{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa41670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd5fe6",
   "metadata": {},
   "source": [
    "# 1 Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be708d5a",
   "metadata": {},
   "source": [
    "Der Datensatz unserer Challenge befasst sich mit der Analyse und dem Vorhersagen von Herzinfarkten. Durch die Analyse von verschiedenen Patientendaten soll frühzeitig ein hohes Herzinfarktrisiko vorhergesagt werden. Dadurch kann rechtzeitig mit einer Behandlung zur Prävention begonnen werden. \n",
    "\n",
    "Kein besonderer Fokus wurde auf die benötigten Ressourcen der künstlichen Intelligenz gelegt. Dies liegt daran, dass ein Menschenleben nicht mit den Ressourcen abzuwiegen ist. Aus diesem Grund geht ebenfalls heraus, dass jedes Prozent der Genauigkeit wichtig ist. Außerdem ist kein besonders hoher Wert auf die Performance der Durchlaufzeit oder ähnlichen Werten gelegt. Aus Darstellungsgründen wurden jedoch an einigen Stellen Annahmen und Vereinfachungen eingebaut.\n",
    "\n",
    "Ein besonderer Fokus der Berechnung soll darauf liegen, dass möglichst wenige Herzinfarkte unentdeckt bleiben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2463e",
   "metadata": {},
   "source": [
    "# 2 Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415b81c",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Collect Initial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022965e7",
   "metadata": {},
   "source": [
    "Link zum Datensatz: https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfdd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Datenset laden\n",
    "fulldata = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d87dac",
   "metadata": {},
   "source": [
    "## 2.2 Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb26df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749794cf",
   "metadata": {},
   "source": [
    "Folgende Variablenerklärungen liefert der Datensatz\n",
    "\n",
    "|Variable|Erklärung|Anmerkung|\n",
    "|:---|:---|:---|\n",
    "age|Age of the patient|\n",
    "sex|Sex of the patient|keine Erklärung, welches Geschlecht wie kodiert ist\n",
    "cp|Chest Pain type chest pain type|Value 1: typical angina<br>Value 2: atypical angina<br>Value 3: non-anginal pain<br>Value 4: asymptomatic\n",
    "trtbps|resting blood pressure (in mm Hg)|Ruheblutdruck\n",
    "chol|cholestoral in mg/dl fetched via BMI sensor|Cholesterinspiegel\n",
    "fbs|(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)|Erhöhter Blutzucker\n",
    "restecg|resting electrocardiographic results|Value 0: normal<br>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)<br>Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "thalachh|maximum heart rate achieved|Maximaler Puls\n",
    "exng|exercise induced angina (1 = yes; 0 = no)|Engegefühl in der Brust bei körperlicher Belastung\n",
    "oldpeak|keine Erklärung vorhanden|\n",
    "slp|keine Erklärung vorhanden|\n",
    "caa|number of major vessels (0-3)|weglassen, da Beschreibung uneindeutig, Skala passt auch nicht [0-3] \n",
    "thall|keine Erklärung vorhanden|\n",
    "output|0 = less chance of heart attack<br> 1 = higher chance of heart attack\n",
    "\n",
    "Für die Spalten ``oldpeak``, ``slp`` und ``thall`` existieren keine Erklärungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18da3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.thall.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0c304",
   "metadata": {},
   "source": [
    "Nach weiterer Recherche können folgende Vermutungen aufgestellt werden. \n",
    "\n",
    "Die Spalte ``thall`` bezieht sich auf die Form der Thalssämie. Dabei stehen die Werte 1-3 für die Art der Krankheit, wohingegen die 0 kein Vorhandensein der Krankheit abbildet.  <br> \n",
    "<a href=\"https://www.netdoktor.de/krankheiten/thalassaemie/#:~:text=Die%20Thalass%C3%A4mie%20oder%20Mittelmeeran%C3%A4mie%20ist,Alpha%2D%20und%20Beta%2DThalass%C3%A4mie.\">Quelle 1</a>\n",
    "<br>\n",
    "<a href=\"https://www.kinderblutkrankheiten.de/erkrankungen/rote_blutzellen/anaemien_blutarmut/thalassaemie/krankheitsformen/\">Quelle 2</a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ffe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.slp.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1dde5",
   "metadata": {},
   "source": [
    "Die Spalte ``slp`` ist die Abkürzung für Slope. Dieser Wert wird beim Auswerten eines EKGs benutzt (<a href=\"https://pubmed.ncbi.nlm.nih.gov/18269981/\">Quelle</a>). Die genaue Kodierung der Daten ist unbekannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c55714",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.oldpeak.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5ee02",
   "metadata": {},
   "source": [
    "Für die Spalte ``oldpeak`` wurde keine Erklärung gefunden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c0b70",
   "metadata": {},
   "source": [
    "## 2.3 Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.isnull().sum()\n",
    "#keine fehlenden Daten vorhanden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999303f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344144f4",
   "metadata": {},
   "source": [
    "Der Anteil zu gefährdeten und nicht gefährdeten Personen liegt bei circa 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e441d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(fulldata['age']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(fulldata, corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(fulldata, hue=\"output\", corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45943709",
   "metadata": {},
   "source": [
    "Durch die beiden Pairplots sind keine einfachen Korrelationen der Variablen ersichtlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb98dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = fulldata.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corrmat, cmap=\"binary\", linewidths=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a1ec4",
   "metadata": {},
   "source": [
    "Auch durch weitere Berechnung wird keine deutliche Korrelation zweier Variablen deutlich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4f70f",
   "metadata": {},
   "source": [
    "# 3 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740b0df",
   "metadata": {},
   "source": [
    "## 3.1 Select Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7c156",
   "metadata": {},
   "source": [
    "Innerhalb des Data Understandings sind unerklärte Spalten entdeckt worden. Da der Datensatz in sich geschlossen ist und keine willkürlichen Daten beinhaltet, können die Daten weiter benutzt werden. Hierdurch wird die Anzahl der verfügbaren Daten möglichst hoch gehalten. In einer realen Anwendung des Algorithmus muss das Data Understanding mit weiterem Fachwissen durchgeführt werden, um die Daten besser zu analysieren und hierdurch bessere Ergebnisse zu berechnen. In unserem Fall wurde Absprache mit einer Medizinstudentin durchgeführt. Hierdurch wurde die Aussagekraft der Daten bestätigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4b317",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b0c56",
   "metadata": {},
   "source": [
    "Da mit einem sauberen Datensatz begonnnen wird ist eine separate Datenbereinigung nicht notwendig. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba6e90",
   "metadata": {},
   "source": [
    "## 3.3 Construct Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278848f",
   "metadata": {},
   "source": [
    "Es werden keine zusätzlichen Berechnungen zur Generierung weiterer Daten durchgefüht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5779655",
   "metadata": {},
   "source": [
    "## 3.4 Integrate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3210b10",
   "metadata": {},
   "source": [
    "Es werden keine weiteren Daten hinzugefügt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab73d6",
   "metadata": {},
   "source": [
    "## 3.5 Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datentypen anpassen\n",
    "\n",
    "#Folgende Werte sind kategorisch, werden bisher im Datensatz jedoch numerisch dargestellt\n",
    "\n",
    "cols = [\n",
    "    \"sex\",\n",
    "    \"cp\",\n",
    "    \"restecg\",\n",
    "    \"slp\",\n",
    "    \"caa\",\n",
    "    \"thall\"\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    fulldata[col] = fulldata[col].astype(\"category\")\n",
    "\n",
    "#Folgende Werte sind boolisch, werden bisher im Datensatz jedoch numerisch dargestellt\n",
    "    \n",
    "cols = [\n",
    "    \"fbs\",\n",
    "    \"exng\",\n",
    "     \"output\"\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    fulldata[col] = fulldata[col].astype(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trennung von Daten und Label\n",
    "\n",
    "data = fulldata.drop(['output'], axis=1)\n",
    "\n",
    "output = fulldata['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "X, y = data, output\n",
    "\n",
    "# split the data into 2 sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252eabca",
   "metadata": {},
   "source": [
    "# 4 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9b868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "result_array = []\n",
    "# warum keine precomputed method?\n",
    "methods = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "k = 10\n",
    "for i, element in enumerate(methods):\n",
    "    method = methods[i] \n",
    "    clf = svm.SVC(kernel=method, C=1)\n",
    "    scores = cross_val_score(clf, data, output, cv=k)\n",
    "    result_array.append((scores.mean(), method))\n",
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba830db",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a SVC and optimize it with a grid search\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, param_grid)\n",
    "clf.fit(data, output)\n",
    "\n",
    "# get the best estimator\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "clf = svm.SVC(kernel=\"linear\", C=1)\n",
    "scores = cross_val_score(clf, data, output, cv=k)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bbd0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(np.array(X_train))\n",
    "X_test_scaled = scaler.transform(np.array(X_test))\n",
    "# Define the black box function to optimize.\n",
    "\n",
    "def black_box_function(C):\n",
    "    k=10\n",
    "    model = svm.SVC(kernel=\"linear\", C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(model, data, output, cv=k)\n",
    "    f = scores.mean()\n",
    "    return f\n",
    "def black_box_function_onlyscore(C):\n",
    "    # C: SVC hyper parameter to optimize for.\n",
    "    #model = SVC(C = C, kernel=\"linear\" )\n",
    "    \n",
    "    model = svm.SVC(kernel=\"linear\", C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #from sklearn.metrics import accuracy_score \n",
    "    #fulldata[\"predicted\"] = model.predict(data) \n",
    "    #f = accuracy_score(y_true = fulldata[\"output\"], y_pred = fulldata[\"predicted\"])\n",
    "    #model.fit(X_train_scaled, y_train)\n",
    "    #model.fit(X_train, y_train)\n",
    "    #y_score = model.decision_function(X_test_scaled)\n",
    "    #y_score = model.decision_function(X_test)\n",
    "    #f = roc_auc_score(y_test, y_score)\n",
    "    #y_score = model.decision_function(X_test)\n",
    "    f = model.score(X_test, y_test)\n",
    "    return f\n",
    "\n",
    "def black_box_functionorigi(C):\n",
    "    # C: SVC hyper parameter to optimize for.\n",
    "    model = SVC(C = C, kernel=\"linear\", random_state=42 )\n",
    "    \n",
    "    #model.fit(X_train_scaled, y_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    #y_score = model.decision_function(X_test_scaled)\n",
    "    y_score = model.decision_function(X_test)\n",
    "    f = roc_auc_score(y_test, y_score)\n",
    "    #y_score = model.decision_function(X_test)\n",
    "    #f = model.score(X_train, y_train)\n",
    "    return f\n",
    "# Set range of C to optimize for.\n",
    "# bayes_opt requires this to be a dictionary.\n",
    "pbounds = {\"C\": [1, 100]}\n",
    "# Create a BayesianOptimization optimizer,\n",
    "# and optimize the given black_box_function.\n",
    "optimizer_acc = BayesianOptimization(f = black_box_function,\n",
    "                                 pbounds = pbounds, verbose = 2\n",
    "                                 )\n",
    "optimizer_acc.maximize(init_points = 10, n_iter = 10)\n",
    "print(\"Best result: {}; f(x) = {}.\".format(optimizer_acc.max[\"params\"], optimizer_acc.max[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_acc.max[\"params\"].get(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "model = svm.SVC(kernel=\"linear\", C=optimizer_acc.max[\"params\"].get(\"C\"))\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, data, output, cv=k)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "testmodel = svm.SVC(C = 1, kernel = \"linear\")\n",
    "testmodel.fit(X_train, y_train)\n",
    "testscores = cross_val_score(testmodel, data, output, cv=k)\n",
    "testscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944b19a",
   "metadata": {},
   "source": [
    "## False Negatives verringern\n",
    "### Überblick verschaffen\n",
    "Für unseren Anwendungsfall sind False Negatives besonders fatal, weshalb sie möglichst gering sein soll.\n",
    "\n",
    "Als False Negatives werden Ergebnisse bezeichnet, die als fälschlicherweise als negativ bewertet wurden, ob wohl der Test in Wahrheit positiv ist.\n",
    "\n",
    "Zunächst werden die Ausgangs-Werte bestimmt.\n",
    "Anschließend wird die Gewichtung verändert.\n",
    "Dadurch werden die Ergebnisse \"pessimistischer\", also es wird eher ein mögliches Risiko ausgegeben.\n",
    "\n",
    "Die für eine bessere Genauigkeit sind mehr Daten notwendig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff51b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = svm.SVC(kernel=\"linear\", C=optimizer_acc.max[\"params\"].get(\"C\"))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(data, suptitle):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "    fig.suptitle(suptitle)\n",
    "    \n",
    "    _ = sns.heatmap(data, cmap ='binary', annot = True, fmt=\"d\", ax=ax[0]) \\\n",
    "        .set(xlabel = \"True label\", ylabel = \"Predicted label\")\n",
    "    \n",
    "    _ = sns.heatmap(data / data.sum(), cmap ='binary', annot = True, fmt=\".2%\", ax=ax[1]) \\\n",
    "        .set(xlabel = \"True label\", ylabel = \"Predicted label\")\n",
    "    \n",
    "    # Zusammenaddiert für convenience\n",
    "    float_formatter = \"{:.2%}\".format\n",
    "    print(\"Richtig klassifiziert:\", float_formatter((data[0][0] + data[1][1]) / data.sum()))\n",
    "    print(\"Falsch klassifiziert:\", float_formatter((data[1][0] + data[0][1]) / data.sum()))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf)\n",
    "plot_confusion_matrix(confusion_matrix(y_train, clf.predict(X_train)), \"Confusion Matrix mit Trainingsdaten\")\n",
    "plot_confusion_matrix(confusion_matrix(y_test, clf.predict(X_test)), \"Confusion Matrix mit Testdaten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebee6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(np.array(X_train))\n",
    "X_test_scaled = scaler.transform(np.array(X_test))\n",
    "# Define the black box function to optimize.\n",
    "\n",
    "def black_box_function(zeroWeight):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zeroWeight: hyper parameter to optimize for\n",
    "    \"\"\"\n",
    "    \n",
    "    # KI trainieren\n",
    "    model = svm.SVC(kernel=\"linear\", C=optimizer_acc.max[\"params\"].get(\"C\"), class_weight={0: zeroWeight, 1: 1})\n",
    "    model.fit(X_train, y_train)\n",
    "    matrix = confusion_matrix(y_train, model.predict(X_train))\n",
    "    \n",
    "    true_negative = matrix[0][0]\n",
    "    true_positive = matrix[1][1]\n",
    "    false_negative = matrix[0][1]\n",
    "    false_negative_relative = false_negative / matrix.sum()\n",
    "    accuracy = (true_positive + true_negative) / matrix.sum()\n",
    "    \n",
    "    # accuracy wird mit berücksichtigt, um zu verhindern, dass alle Herzpatienten sind\n",
    "    gewichtung = .5 # getestet, liefert gute Ergebnisse\n",
    "    f = gewichtung * accuracy + (1 - gewichtung) * (1 - false_negative_relative) # Gegenwert zur 1, weil maximiert wird\n",
    "    return f\n",
    "\n",
    "# Set range of C to optimize for.\n",
    "# bayes_opt requires this to be a dictionary.\n",
    "pbounds = {\"zeroWeight\": [1, 5]}\n",
    "# Create a BayesianOptimization optimizer,\n",
    "# and optimize the given black_box_function.\n",
    "optimizer_weight = BayesianOptimization(f = black_box_function,\n",
    "                                 pbounds = pbounds, verbose = 2\n",
    "                                 )\n",
    "optimizer_weight.maximize(init_points = 10, n_iter = 10)\n",
    "print(\"Best result: {}; f(x) = {}.\".format(optimizer_weight.max[\"params\"], optimizer_weight.max[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0f3ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C = optimizer_acc.max[\"params\"].get(\"C\"), kernel = \"linear\", class_weight={0: optimizer_weight.max[\"params\"].get(\"zeroWeight\"), 1: 1})\n",
    "clf.fit(data, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdff275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_train, clf.predict(X_train)), \"Confusion Matrix mit Trainingsdaten\")\n",
    "plot_confusion_matrix(confusion_matrix(y_test, clf.predict(X_test)), \"Confusion Matrix mit Testdaten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fe718",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k=10\n",
    "model = svm.SVC(C = optimizer_acc.max[\"params\"].get(\"C\"), kernel = \"linear\", class_weight={0: optimizer_weight.max[\"params\"].get(\"zeroWeight\"), 1: 1})\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, data, output, cv=k)\n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
